{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd80I4sF2EPv"
      },
      "source": [
        "#Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8GRl4EsjTCR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!pip install -q transformers datasets seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aGe8Hp2KZF"
      },
      "source": [
        "# Copy data from google cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r85kL9X9F-d"
      },
      "outputs": [],
      "source": [
        "!mkdir i2b2_2014\n",
        "!gsutil -m -q cp -r gs://deid-data/i2b2_2014/train/ i2b2_2014/\n",
        "!gsutil -m -q cp -r gs://deid-data/i2b2_2014/test/ i2b2_2014/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google bucket"
      ],
      "metadata": {
        "id": "jSJm-B6TIs1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKbztZTfEIYp"
      },
      "outputs": [],
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYke8N3LFQrl"
      },
      "outputs": [],
      "source": [
        "!mkdir googleBucketFolder\n",
        "!gcsfuse --implicit-dirs deid-data googleBucketFolder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6muQqEG2O4T"
      },
      "source": [
        "# Clone repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxgDGzgvqezE"
      },
      "outputs": [],
      "source": [
        "!git clone https://<user>:<token>@github.com/alistairewj/transformer-deid.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D8dElStriZ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca065b68-533e-457f-8d43-69db21618653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformer-deid\n"
          ]
        }
      ],
      "source": [
        "cd transformer-deid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z48Et2Fs6eJ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForTokenClassification, BertForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "\n",
        "# local packages\n",
        "from transformer_deid.data import DeidDataset, DeidTask\n",
        "from transformer_deid.evaluation import compute_metrics\n",
        "from transformer_deid.tokenization import assign_tags, encode_tags, split_sequences\n",
        "from transformer_deid.utils import convert_dict_to_native_types\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "    level=logging.INFO\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0mF2pP5f4g3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "def seed_everything(seed: int):    \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    \n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvHfZEzH1oaY"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqoeVt6MtNs5"
      },
      "outputs": [],
      "source": [
        "# specify dataset arguments\n",
        "task_name = 'i2b2_2014'\n",
        "split_long_sequences = True\n",
        "label_transform = 'base'\n",
        "\n",
        "deid_task = DeidTask(\n",
        "    task_name,\n",
        "    #data_dir=f'/home/alistairewj/git/deid-gs/{task_name}',\n",
        "    data_dir=f'../{task_name}',\n",
        "    label_transform=label_transform\n",
        ")\n",
        "\n",
        "train_texts, train_labels = deid_task.train['text'], deid_task.train['ann']\n",
        "split_idx = int(0.8 * len(train_texts))\n",
        "val_texts, val_labels = train_texts[split_idx:], train_labels[split_idx:]\n",
        "train_texts, train_labels = train_texts[:split_idx], train_labels[:split_idx]\n",
        "test_texts, test_labels = deid_task.test['text'], deid_task.test['ann']\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1WKYfs1USC"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QXeg6ly1SV6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# split text/labels into multiple examples\n",
        "# (1) tokenize text\n",
        "# (2) identify split points\n",
        "# (3) output text as it was originally\n",
        "if split_long_sequences:\n",
        "    train_texts, train_labels = split_sequences(\n",
        "        tokenizer, train_texts, train_labels\n",
        "    )\n",
        "    val_texts, val_labels = split_sequences(\n",
        "        tokenizer, val_texts, val_labels\n",
        "    )\n",
        "    test_texts, test_labels = split_sequences(\n",
        "        tokenizer, test_texts, test_labels\n",
        "    )\n",
        "\n",
        "# look at one element of train encodings: transformers.tokenization_utils_base.BatchEncoding\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    is_split_into_words=False,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "val_encodings = tokenizer(\n",
        "    val_texts,\n",
        "    is_split_into_words=False,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")  \n",
        "test_encodings = tokenizer(\n",
        "    test_texts,\n",
        "    is_split_into_words=False,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# use the offset mappings in train_encodings to assign labels to tokens\n",
        "train_tags = assign_tags(train_encodings, train_labels)\n",
        "val_tags = assign_tags(val_encodings, val_labels)\n",
        "test_tags = assign_tags(test_encodings, test_labels)\n",
        "\n",
        "# encodings are dicts with three elements:\n",
        "#   'input_ids', 'attention_mask', 'offset_mapping'\n",
        "# these are used as kwargs to model training later\n",
        "train_tags = encode_tags(train_tags, train_encodings, deid_task.label2id)\n",
        "val_tags = encode_tags(val_tags, val_encodings, deid_task.label2id)\n",
        "test_tags = encode_tags(test_tags, test_encodings, deid_task.label2id)\n",
        "\n",
        "# prepare a dataset compatible with Trainer module\n",
        "train_encodings.pop(\"offset_mapping\")\n",
        "val_encodings.pop(\"offset_mapping\")\n",
        "test_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = DeidDataset(train_encodings, train_tags)\n",
        "val_dataset = DeidDataset(val_encodings, val_tags)\n",
        "test_dataset = DeidDataset(test_encodings, test_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwfGPEVZs7Ei"
      },
      "source": [
        "# Train transformer (skip if loading model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a239jdZD1pCD"
      },
      "outputs": [],
      "source": [
        "model = DistilBertForTokenClassification.from_pretrained(\n",
        "    'distilbert-base-cased', num_labels=len(deid_task.labels)\n",
        ")\n",
        "\n",
        "epochs = 1\n",
        "train_batch_size = 8\n",
        "out_dir = f'../googleBucketFolder/DistilBERTresults{epochs}'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=out_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy='epoch'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "logger.info(\"  Num Epochs = %d\", training_args.num_train_epochs)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "save_location = f'{out_dir}/{task_name}_DistilBert_Model_{epochs}'\n",
        "\n",
        "trainer.save_model(save_location)\n",
        "\n",
        "trainer.evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CePd1CCwPDB"
      },
      "source": [
        "# Run dataset through model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZUUtebtnFa"
      },
      "outputs": [],
      "source": [
        "# predictions, labels, _ = trainer.predict(test_dataset)\n",
        "# predicted_label = np.argmax(predictions, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6vQOsAt3Eq"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WN86y7Ntau8"
      },
      "outputs": [],
      "source": [
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "worksheet = gc.open_by_url('https://docs.google.com/spreadsheets/d/1tc_8g2cqBdt6zEobvMUursLzKtWAn0GBCL-iwGvjudA/edit?usp=sharing').worksheet(\"distilbert\")\n",
        "\n",
        "multi_class_fields = ['AGEprecision', 'AGErecall', 'AGEf1', 'AGEnumber', 'CONTACTprecision', 'CONTACTrecall', 'CONTACTf1', 'CONTACTnumber', 'DATEprecision', 'DATErecall', 'DATEf1', 'DATEnumber', 'IDprecision', 'IDrecall', 'IDf1', 'IDnumber', 'LOCATIONprecision', 'LOCATIONrecall', 'LOCATIONf1', 'LOCATIONnumber', 'NAMEprecision', 'NAMErecall', 'NAMEf1', 'NAMEnumber', 'PROFESSIONprecision', 'PROFESSIONrecall', 'PROFESSIONf1', 'PROFESSIONnumber', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n",
        "binary_fields = ['PHIprecision', 'PHIrecall', 'PHIf1', 'PHInumber', 'overall_precision', 'overall_recall', 'overall_f1', 'overall_accuracy']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A49cGOhoBh-"
      },
      "outputs": [],
      "source": [
        "def flatten_dict(d):\n",
        "    out = {}\n",
        "    for key in d:\n",
        "        if type(d[key]) is dict:\n",
        "            child = flatten_dict(d[key])\n",
        "            for child_key in child:\n",
        "                val = child[child_key]\n",
        "                if isinstance(val, np.int64):\n",
        "                    val = int(val)\n",
        "                out[key + child_key] = val\n",
        "        else:\n",
        "            out[key] = d[key]\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crNqUaZgMmAf"
      },
      "outputs": [],
      "source": [
        "def add_row(epochs, results_multiclass, results_binary, multi_class_fields, binary_fields):\n",
        "    \"\"\"\n",
        "    Add row to worksheet\n",
        "    fields: [epochs] + multi_class_fields + binary_fields\n",
        "    \"\"\"\n",
        "\n",
        "    row = [epochs] + [flatten_dict(results_multiclass).get(field) for field in multi_class_fields] + [flatten_dict(results_binary).get(field) for field in binary_fields]\n",
        "\n",
        "    worksheet.append_row(row, table_range='A1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iou0VykVtx1F"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "metric_dir = \"transformer_deid/token_evaluation.py\"\n",
        "metric = load_metric(metric_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate every checkpoint"
      ],
      "metadata": {
        "id": "lovO2qO2JyTg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2PX6PQJLtee"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def eval_checkpoints(path):\n",
        "    step = int(path.split('-')[-1])\n",
        "    steps_per_epoch = math.ceil(len(train_dataset) / train_batch_size)\n",
        "    epoch = step / steps_per_epoch\n",
        "    model = DistilBertForTokenClassification.from_pretrained(path, num_labels=len(deid_task.labels))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    )\n",
        "    \n",
        "    predictions, labels, _ = trainer.predict(test_dataset)\n",
        "    predicted_label = np.argmax(predictions, axis=2)\n",
        "\n",
        "    results_multiclass = compute_metrics(\n",
        "        predicted_label, labels, deid_task.labels, metric=metric\n",
        "    )\n",
        "    results_binary = compute_metrics(\n",
        "        predicted_label, labels, deid_task.labels, metric=metric, binary_evaluation=True\n",
        "    )\n",
        "    add_row(epoch, results_multiclass, results_binary, multi_class_fields, binary_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9zjnno8nS3H"
      },
      "outputs": [],
      "source": [
        "root = f'../googleBucketFolder/DistilBERTresults{epochs}'\n",
        "checkpoints = [\n",
        "               item for item in os.listdir(root)\n",
        "               if 'checkpoint' in item and os.path.isdir(os.path.join(root, item))\n",
        "               ]\n",
        "for item in tqdm(sorted(checkpoints, key=lambda x: int(x.split('-')[1]))):\n",
        "    eval_checkpoints(os.path.join(root, item))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "deid-distilbert-train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}