{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd80I4sF2EPv"
      },
      "source": [
        "#Initializations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8GRl4EsjTCR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'kind-lab'\n",
        "!gcloud config set project {project_id}\n",
        "\n",
        "!pip install -q transformers datasets seqeval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aGe8Hp2KZF"
      },
      "source": [
        "# Copy from google cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_r85kL9X9F-d"
      },
      "outputs": [],
      "source": [
        "!mkdir i2b2_2014\n",
        "!gsutil -m -q cp -r gs://deid-data/i2b2_2014/train/ i2b2_2014/\n",
        "!gsutil -m -q cp -r gs://deid-data/i2b2_2014/test/ i2b2_2014/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6muQqEG2O4T"
      },
      "source": [
        "# Clone repo  TODO: enter credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxgDGzgvqezE"
      },
      "outputs": [],
      "source": [
        "!git clone https://[username]:[token]@github.com/alistairewj/transformer-deid.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abqRAm5pe1yQ"
      },
      "outputs": [],
      "source": [
        "!git checkout xidev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D8dElStriZ0"
      },
      "outputs": [],
      "source": [
        "cd transformer-deid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6z48Et2Fs6eJ"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForTokenClassification, BertForTokenClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "\n",
        "# local packages\n",
        "from transformer_deid.data import DeidDataset, DeidTask\n",
        "from transformer_deid.evaluation import compute_metrics\n",
        "from transformer_deid.tokenization import assign_tags, encode_tags, split_sequences\n",
        "from transformer_deid.utils import convert_dict_to_native_types\n",
        "\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "    level=logging.INFO\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvHfZEzH1oaY"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqoeVt6MtNs5"
      },
      "outputs": [],
      "source": [
        "# specify dataset arguments\n",
        "task_name = 'i2b2_2014'\n",
        "split_long_sequences = True\n",
        "label_transform = 'base'\n",
        "\n",
        "deid_task = DeidTask(\n",
        "    task_name,\n",
        "    #data_dir=f'/home/alistairewj/git/deid-gs/{task_name}',\n",
        "    data_dir=f'../{task_name}',\n",
        "    label_transform=label_transform\n",
        ")\n",
        "\n",
        "train_texts, train_labels = deid_task.train['text'], deid_task.train['ann']\n",
        "test_texts, test_labels = deid_task.test['text'], deid_task.test['ann']\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1WKYfs1USC"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QXeg6ly1SV6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# split text/labels into multiple examples\n",
        "# (1) tokenize text\n",
        "# (2) identify split points\n",
        "# (3) output text as it was originally\n",
        "if split_long_sequences:\n",
        "    train_texts, train_labels = split_sequences(\n",
        "        train_texts, train_labels, tokenizer\n",
        "    )\n",
        "    test_texts, test_labels = split_sequences(\n",
        "        test_texts, test_labels, tokenizer\n",
        "    )\n",
        "\n",
        "# look at one element of train encodings: transformers.tokenization_utils_base.BatchEncoding\n",
        "train_encodings = tokenizer(\n",
        "    train_texts,\n",
        "    is_split_into_words=False,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")  \n",
        "test_encodings = tokenizer(\n",
        "    test_texts,\n",
        "    is_split_into_words=False,\n",
        "    return_offsets_mapping=True,\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# use the offset mappings in train_encodings to assign labels to tokens\n",
        "train_tags = assign_tags(train_encodings, train_labels)\n",
        "test_tags = assign_tags(test_encodings, test_labels)\n",
        "\n",
        "# encodings are dicts with three elements:\n",
        "#   'input_ids', 'attention_mask', 'offset_mapping'\n",
        "# these are used as kwargs to model training later\n",
        "train_tags = encode_tags(train_tags, train_encodings, deid_task.label2id)\n",
        "test_tags = encode_tags(test_tags, test_encodings, deid_task.label2id)\n",
        "\n",
        "# prepare a dataset compatible with Trainer module\n",
        "train_encodings.pop(\"offset_mapping\")\n",
        "test_encodings.pop(\"offset_mapping\")\n",
        "train_dataset = DeidDataset(train_encodings, train_tags)\n",
        "test_dataset = DeidDataset(test_encodings, test_tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwfGPEVZs7Ei"
      },
      "source": [
        "# Train transformer (skip if loading model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a239jdZD1pCD"
      },
      "outputs": [],
      "source": [
        "model = DistilBertForTokenClassification.from_pretrained(\n",
        "    'distilbert-base-cased', num_labels=len(deid_task.labels)\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "logger.info(\"  Num Epochs = %d\", training_args.num_train_epochs)\n",
        "\n",
        "# log top 5 examples\n",
        "for i in range(min(len(train_dataset), 5)):\n",
        "    input_ids, attention_mask, token_type_ids, label_ids = train_dataset.get_example(\n",
        "        i, deid_task.id2label\n",
        "    )\n",
        "\n",
        "    # convert ids into human interpretable values\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    labels = [\n",
        "        '-100' if l == -100 else deid_task.id2label[l] for l in label_ids\n",
        "    ]\n",
        "\n",
        "    logger.info(\"*** Example %d ***\", i)\n",
        "    logger.info(\"tokens: %s\", \" \".join(tokens))\n",
        "    logger.info(\"labels: %s\", \" \".join(labels))\n",
        "    logger.info(\"input_ids: %s\", \" \".join(map(str, input_ids)))\n",
        "    logger.info(\"label_ids: %s\", \" \".join(map(str, label_ids)))\n",
        "    logger.info(\"input_mask: %s\", \" \".join(map(str, attention_mask)))\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(f'results/{task_name}_DistilBert_Model')\n",
        "\n",
        "trainer.evaluate()\n",
        "\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "predicted_label = np.argmax(predictions, axis=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSDMlc4D_AdZ"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aTst2zq70y6q"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = DistilBertForTokenClassification.from_pretrained(\n",
        "    '../drive/MyDrive/deid/transformer-deid/results/i2b2_2014_DistilBert_Model', num_labels=len(deid_task.labels)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CePd1CCwPDB"
      },
      "source": [
        "# Run dataset through model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXZUUtebtnFa"
      },
      "outputs": [],
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "predictions, labels, _ = trainer.predict(test_dataset)\n",
        "predicted_label = np.argmax(predictions, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo6vQOsAt3Eq"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iou0VykVtx1F"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "metric_dir = \"transformer_deid/token_evaluation.py\"\n",
        "metric = load_metric(metric_dir)\n",
        "results = compute_metrics(\n",
        "    predictions, labels, deid_task.labels, metric=metric\n",
        ")\n",
        "\n",
        "pprint.pprint(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rVFrKID4PXE"
      },
      "source": [
        "# Test example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-9QGvpeepki"
      },
      "outputs": [],
      "source": [
        "cd transformer_deid/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLkwbH0UxPS-"
      },
      "outputs": [],
      "source": [
        "from predict import *\n",
        "text = test_texts[0]\n",
        "deid_text = deid_example(text, model)\n",
        "\n",
        "print(text)\n",
        "print('===================')\n",
        "print(deid_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RwfGPEVZs7Ei"
      ],
      "name": "deid.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
